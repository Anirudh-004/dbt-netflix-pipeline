# Netflix Data Engineering Pipeline (Airflow + DBT + Docker)

![Python](https://img.shields.io/badge/python-3.9-blue)
![Airflow](https://img.shields.io/badge/airflow-2.7-brightgreen)
![DBT](https://img.shields.io/badge/dbt-1.7-orange)
![Docker](https://img.shields.io/badge/docker-compose-blue)
![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)

ğŸ“Š **End-to-end data engineering project** that demonstrates how to build a modern data pipeline using **Apache Airflow, PostgreSQL, DBT, AWS S3, and Docker**.  

This project leverages the [MovieLens dataset](https://grouplens.org/datasets/movielens/) and walks through the entire process of **data ingestion â†’ storage â†’ transformation â†’ testing â†’ analytics-ready tables**.

---

## ğŸ“‘ Table of Contents
- [Project Overview](#-project-overview)
- [Architecture](#-architecture)
- [Project Structure](#-project-structure)
- [Tech Stack](#ï¸-tech-stack)
- [Setup Instructions](#ï¸-setup-instructions)
- [Data Modeling with DBT](#-data-modeling-with-dbt)
- [Example Outputs](#-example-outputs)
- [Key Learnings](#-key-learnings)
- [Future Enhancements](#-future-enhancements)
- [Contributing](#-contributing)
- [License](#-license)

---

## ğŸš€ Project Overview  

- **Dataset Source:** [MovieLens](https://grouplens.org/datasets/movielens/)  
- **Data Lake:** AWS S3 (raw CSV files)  
- **Orchestration:** Apache Airflow (Dockerized)  
- **Data Warehouse:** PostgreSQL (Dockerized)  
- **Transformation:** DBT (with snapshots, tests, fact & dimension models)  
- **Visualization / Monitoring:** pgAdmin (Dockerized)  

The pipeline demonstrates **real-world data engineering practices** such as:  
- Running services with **Docker Compose**  
- **Ingesting raw data** from S3 into PostgreSQL using Airflow  
- **Data modeling with DBT** (dimension, fact, and mart layers)  
- Implementing **tests & macros** for data quality  
- **Slowly Changing Dimension (SCD) Type 2** handling with DBT snapshots  

## ğŸ—ï¸ Architecture  

```text
           +-------------+
           |  MovieLens  |
           |   Dataset   |
           +------+------+   
                  | (CSV upload)
                  v
             +----+----+
             |   AWS   |
             |   S3    |
             +----+----+
                  |
        Ingestion with Airflow
                  |
                  v
        +---------+---------+
        |   PostgreSQL DB   |
        |   (Dockerized)    |
        +---------+---------+
                  |
        Transformation with DBT
                  |
       +----------+----------+
       |  Analytics Schemas  |
       |   (dev: staging,    |
       |   dim, fct, mart)   |
       +----------+----------+

## ğŸ“‚ Project Structure

```text
.
â”œâ”€â”€ Netflix_Data_Pipeline/      # Airflow DAGs for ingestion
â”œâ”€â”€ dbt_project/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ dim/                # Dimension tables
â”‚   â”‚   â”œâ”€â”€ fct/                # Fact tables
â”‚   â”‚   â”œâ”€â”€ mart/               # Aggregated marts
â”‚   â”‚   â””â”€â”€ staging/            # Staging models
â”‚   â”œâ”€â”€ macros/                 # Custom macros for tests
â”‚   â”œâ”€â”€ seeds/                  # CSV-based seed data
â”‚   â”œâ”€â”€ snapshots/              # SCD Type 2 tracking
â”‚   â””â”€â”€ tests/                  # Data quality tests
â”œâ”€â”€ docker-compose.yml          # Orchestration of all services
â””â”€â”€ README.md                   # Project documentation


ğŸ› ï¸ Tech Stack

Apache Airflow â†’ Orchestration (ETL from S3 â†’ PostgreSQL)

PostgreSQL â†’ Data warehouse (schemas: public, dev)

pgAdmin â†’ GUI for exploring database tables

DBT â†’ Transformation layer (staging, dim, fct, mart, tests, snapshots)

Docker Compose â†’ Containerized environment for all services

AWS S3 â†’ Raw data storage

Setup Instructions
1ï¸âƒ£ Clone the repository
git clone https://github.com/Anirudh-004/dbt-netflix-pipeline.git
cd Netflix_Data_Eng_Pipeline

2ï¸âƒ£ Spin up services with Docker Compose
docker-compose up -d

This will start:

Airflow webserver & scheduler

PostgreSQL database

pgAdmin

DBT environment

3ï¸âƒ£ Access the services
- Airflow UI: http://localhost:8080
- pgAdmin: http://localhost:8888/
- Postgres DB: localhost:5432

4ï¸âƒ£ Run the pipeline

Upload MovieLens CSV files to your AWS S3 bucket

Trigger the Airflow DAG â†’ Data will be ingested into PostgreSQL (public schema)

Run DBT transformations to create staging, dimension, fact, and mart models in the dev schema

ğŸ§© Data Modeling with DBT

Staging layer â†’ Light transformations of raw S3 data (in dev.staging)

Dimension layer (dim/) â†’ Descriptive attributes of entities (movies, tags, users, etc.)

Fact layer (fct/) â†’ Transactional/quantitative events (ratings, genome scores, etc.)

Mart layer (mart/) â†’ Aggregated tables for analytics (joins + seed-based enrichments)

Snapshots â†’ Implements SCD Type 2 for tracking historical changes

Tests â†’ Custom DBT macros ensure data quality (e.g., uniqueness, not null)

ğŸ“¸ Example Outputs

âœ… Airflow DAG graph for ingestion

âœ… pgAdmin view of Postgres tables

âœ… DBT lineage graph for transformations

ğŸŒŸ Key Learnings

How to orchestrate a modern data pipeline with Airflow + DBT

Using Docker Compose to spin up an entire data engineering stack

Best practices in data modeling (staging â†’ dim/fct â†’ mart)

Implementing data quality tests and SCD Type 2 snapshots in DBT

ğŸ“Œ Future Enhancements

Add CI/CD with GitHub Actions for DBT testing

Deploy DBT docs for auto-generated documentation

Integrate a BI tool (e.g., Metabase / Superset) for visualization




